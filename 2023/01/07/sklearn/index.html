<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Reggie">





<title>机器学习-sklearn | Reggie&#39;s blog</title>



    <link rel="icon" href="/favicon.jpg">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 6.2.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Reggie&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Reggie&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6;    // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function () {
            tocbot.refresh(obj_merge(tocbot_default_config, { hasInnerContainers: true }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function () {
        tocbot.init(obj_merge(tocbot_default_config, { collapseDepth: 1 }));
    });

    function expandToc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, { collapseDepth: expanded ? 1 : DEPTH_MAX }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">机器学习-sklearn</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Reggie</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">January 7, 2023&nbsp;&nbsp;22:04:09</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="Sklearn库的概述"><a href="#Sklearn库的概述" class="headerlink" title="Sklearn库的概述"></a>Sklearn库的概述</h1><p>sklearn库一共包含6大部分, 分别用于:</p>
<ol>
<li>分类任务<ul>
<li>最近邻算法: neighbors.NearestNeighbors</li>
<li>支持向量机: svm.SVC</li>
<li>朴素贝叶斯: naive_bayes.GaussianNB</li>
<li>决策树: tree.DecisionTreeClassifier</li>
<li>集成方法: ensemble.GaggingClassifier</li>
<li>神经网络: neural_network.MLPClassifier</li>
</ul>
</li>
<li>回归任务<ul>
<li>岭回归: linear_model.Ridge</li>
<li>Lasso回归: linear_model.Lasso</li>
<li>弹性网络: linear_model.ElasticNet</li>
<li>最小角回归: linear_model.Lars</li>
<li>贝叶斯回归: linear_model.BayesianRidge</li>
<li>逻辑回归: linear_model.LogisticRegression</li>
<li>多项式回归: preprocessing.PolynomialFeatures</li>
</ul>
</li>
<li>聚类任务(无监督)<ul>
<li>K-means: cluster.KMeans</li>
<li>AP聚类: cluster.AffinityPropagation</li>
<li>均值漂移: cluster.MeanShift</li>
<li>层次聚类: cluster.AgglomerativeClustering</li>
<li>DBSCAN: cluster.DBSCAN</li>
<li>BIRCH: cluster.Birch</li>
<li>谱聚类: cluster.SpectralClustering</li>
</ul>
</li>
<li>降维任务(dimension reduction)(无监督)<ul>
<li>主成分分析: decomposition.PCA</li>
<li>截断SVD和LSA: decomposition.TruncatedSVD</li>
<li>字典学习: decomposition.SparseCoder</li>
<li>因子分析: decomposition.FactorAnalysis</li>
<li>独立成分分析: decomposition.FastICA</li>
<li>非负矩阵分解: decomposition.NMF</li>
<li>LDA: decomposition.LatentDirichletAllocation</li>
</ul>
</li>
<li>模型选择</li>
<li>数据预处理</li>
</ol>
<h1 id="Sklearn数据集"><a href="#Sklearn数据集" class="headerlink" title="Sklearn数据集"></a>Sklearn数据集</h1><h2 id="数据集概述"><a href="#数据集概述" class="headerlink" title="数据集概述"></a>数据集概述</h2><p>所有的内置数据集位于 sklearn.datasets</p>
<table>
<thead>
<tr>
<th>数据集</th>
<th>适合任务</th>
<th>加载函数</th>
<th>规模</th>
</tr>
</thead>
<tbody><tr>
<td>波士顿房价预测</td>
<td>回归</td>
<td>load_boston()</td>
<td>506 * 13</td>
</tr>
<tr>
<td>鸢尾花数据集</td>
<td>多分类任务 (3个类别)</td>
<td>load_iris()</td>
<td>150 * 4</td>
</tr>
<tr>
<td>手写数字数据集</td>
<td>分类</td>
<td>load_digits()</td>
<td>1797 * 64</td>
</tr>
<tr>
<td>糖尿病数据集</td>
<td>回归</td>
<td>load_diabetes()</td>
<td>442 * 10</td>
</tr>
<tr>
<td>红酒识别数据集</td>
<td>分类</td>
<td>load_wine()</td>
<td>178 * 13</td>
</tr>
<tr>
<td>威斯康星州乳腺癌数据集</td>
<td>二分类</td>
<td>load_breast_cancer</td>
<td>569 * 30</td>
</tr>
<tr>
<td>Olivetti脸部数据集</td>
<td>降维</td>
<td>fetch_olivetti_faces()</td>
<td></td>
</tr>
<tr>
<td>新闻分类数据集</td>
<td>分类</td>
<td>fetch_20newsgroups()</td>
<td></td>
</tr>
<tr>
<td>带标签的人脸数据集</td>
<td>分类, 降维</td>
<td>fetch_lfw_people()</td>
<td></td>
</tr>
<tr>
<td>路透社新闻预料数据集</td>
<td>分类</td>
<td>fetch_rev1()</td>
<td>804414 * 47236</td>
</tr>
</tbody></table>
<h3 id="波士顿房价预测"><a href="#波士顿房价预测" class="headerlink" title="波士顿房价预测"></a>波士顿房价预测</h3><p>包含了506组数据, 每个数据有13个特征和1个目标值.</p>
<p>特征和目标值:</p>
<ol>
<li>CRIM:城镇人均犯罪率</li>
<li>ZN: 住宅用地超过25000 sq.ft.的比例</li>
<li>INDUS: 城镇非零售商用土地的比例</li>
<li>CHAS:查理斯河空变量(如果边界是河流, 则为1, 否则为0)</li>
<li>NOX:一氧化氮浓度</li>
<li>RM:住宅平均房间数</li>
<li>AGE: 1940年之前建成的自用房屋比例</li>
<li>DIS: 到波士顿五个中心区域的加权距离</li>
<li>RAD:辐射性公路的接近指数</li>
<li>TAX:每10000美元的全值财产税率</li>
<li>PTRATIO:城镇师生比例</li>
<li>B: 1000(Bk-0.63)^2, 其中Bk指代城镇中黑人的比例</li>
<li>LSTAT:人口中地位低下者的比例</li>
<li>MEDV:自主房的平均房价, 以千美元计</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_boston</span><br><span class="line"></span><br><span class="line">dataset = load_boston() # 只返回特征数据, 即返回(506, 13)</span><br><span class="line">print(dataset.data.shape) # (506, 13)</span><br><span class="line"></span><br><span class="line">print(dataset.feature_names)</span><br><span class="line"></span><br><span class="line">features, target = load_boston(return_X_y=True) # 参数值默认为False, 只返回特征, 不返回目标值</span><br><span class="line">features.data.shape --&gt; (506, 13)   target.data.shape --&gt; (506,)</span><br></pre></td></tr></table></figure>

<h3 id="鸢尾花数据集"><a href="#鸢尾花数据集" class="headerlink" title="鸢尾花数据集"></a>鸢尾花数据集</h3><p><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset">https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset</a></p>
<p>鸢尾花数据集采集的是鸢尾花的测量数据以及其所属的类别.测量数据包括:</p>
<ol>
<li>萼片长度</li>
<li>萼片宽度</li>
<li>花瓣长度</li>
<li>花瓣宽度</li>
</ol>
<p>类别一共分为三类:</p>
<ol>
<li>Iris Setosa</li>
<li>Iris Versicolour</li>
<li>Iris Virginica</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line"></span><br><span class="line">iris_dataset = load_iris()</span><br><span class="line">print(iris_dataset.data.shape) # (150, 4)</span><br><span class="line"></span><br><span class="line">print(iris_dataset.feature_names)</span><br><span class="line"></span><br><span class="line">iris_features, iris_class = load_iris(return_X_y=True)</span><br><span class="line">print(iris_features.data.shape) # (150, 4)</span><br><span class="line">print(iris_class.data.shape) # (150,)</span><br></pre></td></tr></table></figure>

<h3 id="手写数字数据集"><a href="#手写数字数据集" class="headerlink" title="手写数字数据集"></a>手写数字数据集</h3><p><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/datasets/toy_dataset.html#optical-recognition-of-handwritten-digits-dataset">https://scikit-learn.org/stable/datasets/toy_dataset.html#optical-recognition-of-handwritten-digits-dataset</a></p>
<p>手写数字数据集包括1797个0-9的手写数字数据, 每个数字由 $8*8$ 大小的矩阵构成, 矩阵中值的范围是 0-16, 代表颜色的深度</p>
<p>两个参数:<br>    - return_X_y<br>    - n_class: 表示返回数据的类别数, 比如n_class&#x3D;5, 表示返回0-4的数据样本</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_digits</span><br><span class="line"></span><br><span class="line">digits_dataset = load_digits()</span><br><span class="line">print(digits_dataset.data.shape) # (1797, 64)</span><br><span class="line"></span><br><span class="line">print(digits_dataset.feature_names)</span><br><span class="line"></span><br><span class="line">digits_features, digits_class = load_digits(return_X_y=True, n_class=6)</span><br><span class="line">print(digits_features.data.shape) # (1083, 64)</span><br><span class="line">print(digits_class.data.shape) # (1083,)</span><br></pre></td></tr></table></figure>

<h3 id="糖尿病数据集"><a href="#糖尿病数据集" class="headerlink" title="糖尿病数据集"></a>糖尿病数据集</h3><p><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset">https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset</a></p>
<p>收集了442名糖尿病患者数据, 10个属性值, 分别是:</p>
<ol>
<li>Age 年龄</li>
<li>Sex 性别</li>
<li>body mass index 体质指数</li>
<li>Average Blood Pressure 平均血压</li>
<li>S1 血清总胆固醇</li>
<li>S2 低密度脂蛋白</li>
<li>S3 高密度脂蛋白</li>
<li>S4 总胆固醇 &#x2F; 高密度脂蛋白</li>
<li>S5 可能是血清甘油三酯水平的日志</li>
<li>S6 血糖水平</li>
</ol>
<p>Target是一年后患疾病的定量指标, 适用于回归任务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_diabetes</span><br><span class="line"></span><br><span class="line">diabetes_dataset = load_diabetes()</span><br><span class="line">print(diabetes_dataset.data.shape) # (442, 10)</span><br><span class="line"></span><br><span class="line">print(diabetes_dataset.feature_names)</span><br><span class="line"></span><br><span class="line">diabetes_features, diabetes_class = load_diabetes(return_X_y=True)</span><br><span class="line">print(diabetes_features.data.shape) # (442, 10)</span><br><span class="line">print(diabetes_class.data.shape) # (1442,)</span><br></pre></td></tr></table></figure>

<h3 id="红酒识别数据集"><a href="#红酒识别数据集" class="headerlink" title="红酒识别数据集"></a>红酒识别数据集</h3><p><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-recognition-dataset">https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-recognition-dataset</a></p>
<p>共包含 178 条数据, 每条数据有13个属性值:</p>
<ol>
<li>Alcohol 酒精</li>
<li>Malic acid 苹果酸</li>
<li>Ash </li>
<li>Alcalinity of ash</li>
<li>Magnesium 镁</li>
<li>Total phenols 总酚</li>
<li>Flavanoids 类黄酮</li>
<li>Nonflavanoid phenols 非黄酮类酚</li>
<li>Proanthocyanins 原花青素</li>
<li>Color intensity 色彩强度</li>
<li>Hue 色调</li>
<li>OD280&#x2F;OD315</li>
<li>Proline 脯氨酸</li>
</ol>
<p>共三个类别: 红酒的三个档次（分别有59，71，48个样本）— class_0 (59), class_1 (71), class_2 (48)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_wine</span><br><span class="line"></span><br><span class="line">wine_dataset = load_wine()</span><br></pre></td></tr></table></figure>

<h3 id="威斯康星州乳腺癌数据集"><a href="#威斯康星州乳腺癌数据集" class="headerlink" title="威斯康星州乳腺癌数据集"></a>威斯康星州乳腺癌数据集</h3><p><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-wisconsin-diagnostic-dataset">https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-wisconsin-diagnostic-dataset</a></p>
<p>569条数据, 30个属性, 212 - 恶性(Malignant)， 357 - 良性(Benign)<br>属性:<br>[‘mean radius’ ‘mean texture’ ‘mean perimeter’ ‘mean area’<br> ‘mean smoothness’ ‘mean compactness’ ‘mean concavity’<br> ‘mean concave points’ ‘mean symmetry’ ‘mean fractal dimension’<br> ‘radius error’ ‘texture error’ ‘perimeter error’ ‘area error’<br> ‘smoothness error’ ‘compactness error’ ‘concavity error’<br> ‘concave points error’ ‘symmetry error’ ‘fractal dimension error’<br> ‘worst radius’ ‘worst texture’ ‘worst perimeter’ ‘worst area’<br> ‘worst smoothness’ ‘worst compactness’ ‘worst concavity’<br> ‘worst concave points’ ‘worst symmetry’ ‘worst fractal dimension’]</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_breast_cancer</span><br><span class="line"></span><br><span class="line">dataset = load_breast_cancer()</span><br><span class="line"></span><br><span class="line">features, n_class = load_breast_cancer(return_X_y=True)</span><br></pre></td></tr></table></figure>

<h1 id="非监督学习"><a href="#非监督学习" class="headerlink" title="非监督学习"></a>非监督学习</h1><h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><p>clustering, 聚类就是根据数据的相似性将数据分为多类的过程. 估算两个不同样本之间的相似性通常就是计算两个样本之间的距离. 使用不同方法计算样本间的距离会关系到聚类结果的好坏.</p>
<p>聚类试图将数据集中的样本划分为若干个不相交的子集, 每个子集称为一个 “簇 (cluster)”. 每个簇可能对应于一些潜在的概念(类别). 在聚类过程中, 算法仅仅能够识别出数据是否应该属于同一个簇, 形成簇结构, 至于每个簇对应的概念语义, 聚类算法是无能为力的, 需要人为来把握.</p>
<h3 id="sklearn-cluster"><a href="#sklearn-cluster" class="headerlink" title="sklearn.cluster"></a>sklearn.cluster</h3><table>
<thead>
<tr>
<th>算法</th>
<th>参数</th>
<th>可扩展性</th>
<th>相似性度量</th>
</tr>
</thead>
<tbody><tr>
<td>K-means</td>
<td>聚类个数</td>
<td>大规模数据</td>
<td>点间距离</td>
</tr>
<tr>
<td>DBSCAN</td>
<td>领域大小</td>
<td>大规模数据</td>
<td>点间距离</td>
</tr>
<tr>
<td>Gaussian Mixtures</td>
<td>聚类个数等</td>
<td>复杂度高, 不适合处理大规模数据</td>
<td>马氏距离</td>
</tr>
<tr>
<td>Birch</td>
<td>分支因子, 阈值等</td>
<td>大规模数据</td>
<td>欧式距离</td>
</tr>
</tbody></table>
<h3 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h3><p>以K为参数, 把n个对象分为k个簇, 使簇内具有较高的相似度, 而簇间的相似都较低.</p>
<p>聚类过程:</p>
<ol>
<li>随机选择K个点作为初始的聚类中心</li>
<li>对于剩余的点, 根据其和聚类中心的距离, 将其归入最近的簇</li>
<li>对每个簇, 计算所有点的均值作为新的聚类中心</li>
<li>重复2,3直到所有的聚类中心不再发生改变</li>
</ol>
<p>sklearn K-means参数和方法:</p>
<ul>
<li>n_clusters: 指定聚类中心的个数, 一般只需给出该参数即可</li>
<li>init: 初始聚类中心的初始化方法, 默认为k-means++</li>
<li>max_iter: 最大的迭代次数, 默认值为300</li>
<li>data: 加载的数据</li>
<li>label: 聚类后各数据所属的标签</li>
<li>fit_predict(): 计算簇中心以及为簇分配序号</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.cluster import KMeans</span><br><span class="line"></span><br><span class="line"># 1. 加载数据集</span><br><span class="line">features, labels = load_iris(return_X_y=True)</span><br><span class="line"># 2. 实例化K-means算法</span><br><span class="line">km = KMeans(n_clusters=3)</span><br><span class="line"># 3. 使用fit_predict()方法进行计算</span><br><span class="line">label = km.fit_predict(features)</span><br></pre></td></tr></table></figure>

<p>K-means需要指定簇的个数, 为了达到好的实验结果, 需要进行多次尝试才能取到最优K值. 而像层次聚类的算法, 就无需指定K值, 只要给定限制条件, 就能自动地得到类别数k.</p>
<h3 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h3><p>基于密度的聚类算法， 聚类的时候不需要指定簇的个数， 最终簇的个数也不确定</p>
<p>DBSCAN将数据点分为三类：</p>
<ul>
<li>核心点：在半径Eps内含有超过MinPts数目的点</li>
<li>边界点：在半径Eps内点的数量小于MinPts， 但是落在核心点的领域内</li>
<li>噪音点：既不是核心点， 也不是边界点的点, 标签为-1</li>
</ul>
<p>聚类过程：</p>
<ol>
<li>将所有的点标记为核心点， 边界点和噪声点</li>
<li>删除噪声点</li>
<li>为距离在Eps内的所有核心点之间赋予一条边</li>
<li>每组连通的核心点形成一个簇</li>
<li>将每个边界点指派到一个与之关联的核心点的簇中</li>
</ol>
<p>sklearn DBSCAN参数和方法:</p>
<ul>
<li>eps: 两个样本被看作邻居节点的最大距离</li>
<li>min_samples: 簇的样本数</li>
<li>metric: 计算距离方式</li>
<li>fit(X): 使用DBSCAN在数据X上进行训练, 可使用db.labels_来获取簇标签</li>
<li></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import DBSCAN</span><br><span class="line"></span><br><span class="line">DBSCAN(eps=0.5, min_samples=5, metric=&#x27;euclidean&#x27;)</span><br></pre></td></tr></table></figure>

<h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><p>降维是在保证数据所具有的代表性特性或者分布的情况下, 将高维的数据转化为低维数据的过程. 可用于数据的可视化, 精简数据</p>
<h3 id="sklearn-decomposition"><a href="#sklearn-decomposition" class="headerlink" title="sklearn.decomposition"></a>sklearn.decomposition</h3><table>
<thead>
<tr>
<th>算法</th>
<th>参数</th>
<th>可扩展性</th>
<th>适用任务</th>
</tr>
</thead>
<tbody><tr>
<td>PCA</td>
<td>所降维度</td>
<td>大规模数据</td>
<td>信号处理</td>
</tr>
<tr>
<td>FastICA</td>
<td>所降维度</td>
<td>超大规模数据</td>
<td>图形图像特征提取</td>
</tr>
<tr>
<td>NMF</td>
<td>所降维度</td>
<td>大规模数据</td>
<td>图形图像特征提取</td>
</tr>
<tr>
<td>LDA</td>
<td>多降维度</td>
<td>大规模数据</td>
<td>文本数据, 主题挖掘</td>
</tr>
</tbody></table>
<h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h3><p>主成分分析(Principal Component Analysis, PCA) 是最常用的一种降维方法, 通常用于高维数据集的探索与可视化, 还可以用作数据压缩和预处理</p>
<p>PCA可以把具有相关性的高维变量合并为线性无关的的低维变量, 成为主成分. 主成分能够尽可能保留原始数据的信息</p>
<p>原理:<br>    矩阵的主成分就是其协方差矩阵对应的特征向量, 按照对用的特征值大小进行排序, 最大的特征值就是第一主成分, 其次是第二主成分, 以此类推.</p>
<p>过程: – 西瓜书 </p>
<ol>
<li>对所有的样本进行中心化: $x_i \leftarrow x_i - \frac{1}{m} \sum_{i&#x3D;1}^m x_i$</li>
<li>计算样本的协方差矩阵 $XX^T$</li>
<li>对协方差矩阵做特征值分解</li>
<li>取最大的d个特征值对应的特征向量 $w_1, w_2, w_3, …, w_d$</li>
<li>输出投影矩阵 $W &#x3D; (w_1, w_2, w_3, …, w_d)$</li>
</ol>
<h4 id="sklearn中的主成分分析"><a href="#sklearn中的主成分分析" class="headerlink" title="sklearn中的主成分分析"></a>sklearn中的主成分分析</h4><p>sklearn.decomposition.PCA</p>
<p>参数:</p>
<ul>
<li>n_components: 指定主成分的个数, 即降维后的维度个数</li>
<li>svd_solver: 设置特征值分解方法: 默认为auto, 其他选择有[full, arpack, randomized]</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.decomposition import PCA</span><br><span class="line"></span><br><span class="line"># 1. 加载iris数据集</span><br><span class="line">features, labels = load_iris(return_X_y=True)</span><br><span class="line"># 2. 降维</span><br><span class="line"># 2.1 构造PCA实例, 并指定维度为2</span><br><span class="line">pca = PCA(n_components=2)</span><br><span class="line"># 2.2 进行降维</span><br><span class="line">reduced_features = pca.fit_transform(features)</span><br><span class="line"># 3. 保存</span><br><span class="line">red_x, red_y = [], []</span><br><span class="line">blue_x, blue_y = [], []</span><br><span class="line">green_x, green_y = [], []</span><br><span class="line"></span><br><span class="line">for i in range(len(reduced_features)):</span><br><span class="line">    if labels[i] == 0:</span><br><span class="line">        red_x.append(reduced_features[i][0])</span><br><span class="line">        red_y.append(reduced_features[i][1])</span><br><span class="line">    elif labels[i] == 1:</span><br><span class="line">        blue_x.append(reduced_features[i][0])</span><br><span class="line">        blue_y.append(reduced_features[i][1])</span><br><span class="line">    else:</span><br><span class="line">        green_x.append(reduced_features[i][0])</span><br><span class="line">        green_y.append(reduced_features[i][1])</span><br><span class="line"># 4. 可视化</span><br><span class="line">plt.scatter(red_x, red_y, c=&#x27;r&#x27;, marker=&#x27;x&#x27;)</span><br><span class="line">plt.scatter(blue_x, blue_y, c=&#x27;b&#x27;, marker=&#x27;D&#x27;)</span><br><span class="line">plt.scatter(green_x, green_y, c=&#x27;g&#x27;, marker=&#x27;.&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img src="/2023/01/07/sklearn/%E9%B8%A2%E5%B0%BE%E8%8A%B1%E9%99%8D%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.png" class="" title="鸢尾花数据集降维可视化">

<h3 id="NMF-非负矩阵分解"><a href="#NMF-非负矩阵分解" class="headerlink" title="NMF 非负矩阵分解"></a>NMF 非负矩阵分解</h3><p>非负矩阵分解(Non-negative Matrix Factorization)是在矩阵中所有元素均为非负数约束条件之下的矩阵分解方法</p>
<p>基本思想:<br>    给定一个非负矩阵V, NMF能够找到一个非负矩阵W和一个非负矩阵H, 使得矩阵W和H的乘积近似等于矩阵V中的值, i.e.,<br>    $$V_{nm} &#x3D; W_{nm} * H_{nm}$$</p>
<p>NMF的目标就是最小化W矩阵和H矩阵乘积和原始矩阵V之间的差别, i.e., </p>
<p>$$ argmin \frac{1}{2}||X-WH||^2 &#x3D; \frac{1}{2} \sum_{ij}(X_{ij} - WH_{ij})^2 $$</p>
<p>W矩阵: 基础图像矩阵, 相当于从原始矩阵V中抽取出来的特征.</p>
<p>H矩阵: 系数矩阵</p>
<p>NMF可广泛地应用于图像分析, 文本挖掘和语音处理等领域</p>
<h4 id="sklearn中的NMF"><a href="#sklearn中的NMF" class="headerlink" title="sklearn中的NMF"></a>sklearn中的NMF</h4><p>需要使用sklearn.decomposition.NMF</p>
<p>参数:</p>
<ul>
<li>n_components: 用于指定分解后矩阵的单个维度k</li>
<li>init: W矩阵和H矩阵的初始化方式, 默认为’nndsvdar’</li>
</ul>
<img src="/2023/01/07/sklearn/NMF_W_H.png" class="" title="非负矩阵分解WH">

<p>使用NMF对人脸进行特征提取</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_olivetti_faces</span><br><span class="line">from sklearn.decomposition import NMF</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from numpy.random import RandomState</span><br><span class="line"></span><br><span class="line"># 加载人脸数据集</span><br><span class="line">face_data = fetch_olivetti_faces(shuffle=True, random_state=RandomState(0))</span><br><span class="line"># 展示6张图像</span><br><span class="line">print()</span><br><span class="line">fig = plt.figure(figsize=(15, 8), dpi=80)</span><br><span class="line">for i in range(6):</span><br><span class="line">    ax = fig.add_subplot(2, 3, i+1)</span><br><span class="line">    ax.imshow(face_data.data[i].reshape((64, 64)), cmap=&#x27;gray&#x27;)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"># 实例化NMF</span><br><span class="line">nmf = NMF(n_components=6)</span><br><span class="line">nmf.fit(face_data.data)</span><br><span class="line">components_ = nmf.components_</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(15, 8), dpi=80)</span><br><span class="line">for i in range(6):</span><br><span class="line">    ax = fig.add_subplot(2, 3, i+1)</span><br><span class="line">    ax.imshow(components_[i].reshape((64, 64)), cmap=&#x27;gray&#x27;)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h1 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h1><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><h4 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h4><table>
<thead>
<tr>
<th align="center">真实情况</th>
<th align="center">预测结果</th>
<th align="center"></th>
</tr>
</thead>
<tbody><tr>
<td align="center"></td>
<td align="center">正例</td>
<td align="center">反例</td>
</tr>
<tr>
<td align="center">正例</td>
<td align="center">TP (真正例)</td>
<td align="center">FN (假反例)</td>
</tr>
<tr>
<td align="center">反例</td>
<td align="center">FP (假正例)</td>
<td align="center">TN (真反例)</td>
</tr>
</tbody></table>
<p>TP &amp; TN是预测正确的.</p>
<ul>
<li><p>查准率<br>$$ 查准率(precision) &#x3D; 预测为正例的样本中, 真正为正例的占比 &#x3D; \frac{TP}{TP + FP} $$<br>假设你挑选了10个你认为是好的西瓜, 这里面真正好瓜所占的比例就是查准率</p>
</li>
<li><p>查全率(召回率)<br>$$ 查全率(Recall) &#x3D; 所有的正样本中有多少被挑选出来 &#x3D; \frac{TP}{TP + FN} $$<br>假设一共有100个西瓜, 其中20个是好瓜. 在你挑选的10个瓜全是好瓜, 则查全率为10&#x2F;20 &#x3D; 50%</p>
</li>
<li><p>F1<br>$$ F1 &#x3D; \frac{2 * P * R}{P + R} &#x3D; \frac{2 * TP}{样本总数 + TP - TN} $$ or</p>
</li>
</ul>
<p>$$ \frac{1}{F1} &#x3D; \frac{1}{2}(\frac{1}{P} + \frac{1}{R}) $$</p>
<p>F1本质是召回率R和查准率R两者的调和平均.</p>
<p>若对召回率和查准率有不同的偏好, 可以加入一个 $\beta$ 参数, i.e.,</p>
<p>$$ \frac{1}{F_{\beta}} &#x3D; \frac{1}{1 + \beta^2}(\frac{1}{P} + \frac{\beta^2}{R}) $$</p>
<p>若 $\beta &gt; 1$ 时查全率有更大的影响; $\beta &lt; 1$ 时查准率有更大的影响.</p>
<ul>
<li>准确率<br>$$ 准确率(Accuracy) &#x3D; \frac{预测正确的样本数}{总样本数} $$</li>
</ul>
<h4 id="统计假设检验-hypothesis-test"><a href="#统计假设检验-hypothesis-test" class="headerlink" title="统计假设检验(hypothesis test)"></a>统计假设检验(hypothesis test)</h4><p>得到评估结果并不能直接就说这个模型怎么怎么样, 因为:</p>
<ol>
<li>测试性能 !&#x3D; 泛化误差</li>
<li>测试性能随着测试集的变化而变化</li>
<li>很多机器学习算法本身有一定的随机性(比如深度学习初始化)</li>
</ol>
<p>统计假设检验:<br><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1R44y1o749?p=15&amp;vd_source=feb9d077144bcde433eb53975f81efdc">https://www.bilibili.com/video/BV1R44y1o749?p=15&amp;vd_source=feb9d077144bcde433eb53975f81efdc</a></p>
<ol>
<li>交叉验证 t 检验(基于成对t检验)<ul>
<li>K折交叉验证</li>
</ul>
</li>
<li>McNemar检验(基于列联表, 卡方检验)</li>
</ol>
<h3 id="经典分类算法"><a href="#经典分类算法" class="headerlink" title="经典分类算法"></a>经典分类算法</h3><table>
<thead>
<tr>
<th align="center">算法</th>
<th align="center">优点</th>
<th align="center">缺点</th>
</tr>
</thead>
<tbody><tr>
<td align="center">KNN</td>
<td align="center">精度高, 对异常值不敏感, 无数据输入假定</td>
<td align="center">计算复杂度高, 空间复杂度高</td>
</tr>
<tr>
<td align="center">决策树</td>
<td align="center">计算复杂度低, 结果易于理解, 对缺失值不敏感, 可以处理不相关特征数据</td>
<td align="center">可能会产生过度匹配问题</td>
</tr>
<tr>
<td align="center">随机森林</td>
<td align="center">能处理多特征数据, 不需要做特征选择, 容易并行, 速度快, 可以可视化, 便于分析</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">朴素贝叶斯</td>
<td align="center">数据较少时也适用, 可以处理多类别问题</td>
<td align="center">对输入的数据比较敏感</td>
</tr>
</tbody></table>
<h4 id="K近邻分类器-KNN"><a href="#K近邻分类器-KNN" class="headerlink" title="K近邻分类器(KNN)"></a>K近邻分类器(KNN)</h4><p>KNN是一种典型的基于邻居的分类算法(Neighbors-based classification). 这类型的算法的核心在于距离的计算. 他们并不会在内部去构造一个模型, 而是仅仅对训练数据进行简单的存储. 分类器的任务就是去为新样本寻找最近的邻居, 并统计邻居节点的投票数据, 根据少服从多来决定新样本的类别. </p>
<p>KNN: 通过计算待分类数据点与已有数据集中所有数据点的距离. 取距离最小的前K个点, 根据少数服从多数的原则, 将这个数据点划分为出现次数最多的那个类别.</p>
<p>分类过程:</p>
<ul>
<li>KNN是一个典型的监督学习. 当有新的数据进入时, KNN首先会计算该样例与所有训练数据样本的距离, 记为 $ D &#x3D; {d_1, d_2, …, d_n} $.</li>
<li>然后对距离集合D进行排序, 并选择最小的K个值</li>
<li>最后按照少数服从多数的原则, 将该样本归于出现次数较多的类别.</li>
</ul>
<blockquote>
<p>Sklean中的KNN</p>
<ul>
<li>sklearn.neighbors.KNeighborsClassifier(n_neighbors&#x3D;5)</li>
<li>参数有:<br>1. n_neighbors: int, 即K的值, 默认为5. K值较大, 相当于使用较大邻域进行预测, 可以减小估计误差, 但是距离较远的样本也会对预测起作用, 导致预测错误. 相反若K较小, 如果邻居正好是噪声点, 会导致过拟合.<br>2. weights: 设置选中的K个点对分类结果的影响的权重, 默认是平均权重”uniform”. 还有 “distance” 表示越近的点权重越高, 也可以使用自定义的权重算法.<br>3. algorithm: 设置用于计算临近点的方法, 因为当数据量很大的情况下计算当前点和所有点的距离再选择出K个点. 这个过程是比较费时的. 选项有: ball_tree, kd_tree和brute, 分别代表不同的邻居寻找方法, 默认值为auto, 根据训练数据自动选择较好的算法.</li>
<li>方法有:<br>1. fit(X, y): 将训练数据X和标签y送入分类器进行学习<br>2. predict($X^\prime$): 对未知样本$X^\prime$进行分类预测, 返回预测标签<br>在KNN中, 一般我们会选择较小的K, 并使用交叉验证, 选择最优k值.</li>
</ul>
</blockquote>
<h4 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h4><p>决策树是一种树形结构的分类器, 通过顺序询问分类点的属性决定分类点最终的类别. 通常根据特征的信息增益或其他指标, 构建一颗决策树. 在分类时, 只需要按照决策树中的节点依次进行判断, 即可得到样本所属类别.</p>
<p>两种节点:<br>    1. 决策节点(decision points)<br>    2. 叶子节 点, 终止点(terminating points)</p>
<blockquote>
<p>决策树的构造(训练过程):<br>根据熵或者基尼系数从根节点开始选择决策特征并进行分支选择.</p>
</blockquote>
<p>熵(entropy):<br>    - 熵是表示系统内的混乱程度. 比如杂货市场里面啥都有, 比较混乱, 熵值较大. 专卖店只售卖一种牌子的手机, 比较稳定, 熵值较小.<br>    - 熵在分类中的作用就是评估一个特征, 若将这个特征作为决策特征进行分类, 分类后的各个分支的熵(混乱程度)若比较小, 稳定, 说明这个特征就比较适合当前的分类数据. 若分类结束后, 各个分支中比较乱, 说明这个特征并没有完全将数据分开.<br>    - $ H &#x3D; -\sum_{i&#x3D;1}^np(x_i)log p(x_i) $</p>
<p> 信息增益 ID3 (information gain):<br>    - 在划分数据集之前和之后信息发生的变化称为信息增益. 也就是划分前的熵 $H_0$ 和划分之后的熵 $H_1$ 的差值就是信息增益.<br>    - 信息增益越大, 特征越好<br>    - 假设元数据集中有14条数据, 4个特征, 分别是天气(情, 多云, 下雨), 温度, 湿度, 风. label是关于是否出去打球. label中一共有9个yes, 5个no. 那么 $$H_0 &#x3D; -\frac{9}{14}log_2\frac{9}{14}-\frac{5}{14}log_2\frac{5}{14} &#x3D; 0.940$$ 按照天气划分, 其中情时label为2个yes, 3个no; 多云时4个yes, 下雨时3个yes,2个no. 那么 $$H_{sunny} &#x3D; -\frac{2}{5}log_2\frac{2}{5}-\frac{3}{5}log_2\frac{3}{5} &#x3D; 0.971$$ $$H_{overcast} &#x3D; 0$$ $$H_{rainy} &#x3D; -\frac{3}{5}log_2\frac{3}{5}-\frac{2}{5}log_2\frac{2}{5} &#x3D; 0.971$$ 则, 分类后的熵 $$ H_1 &#x3D; \frac{5}{14} H_{sunny} + \frac{4}{14} H_{overcast} + \frac{5}{14} H_{rainy} &#x3D; 0.693$$ 按照天气划分的信息增益 $$ ingi_{weather} &#x3D; H_0 - H_1 &#x3D; 0.940 - 0.693 &#x3D; 0.247 $$ 其他特征的信息增益同理计算, 得到(0.029, 0.152, 0.048). 因为0.247 &gt; 0.152 &gt; 0.048 &gt; 0.029, 所以根节点就是按照0.247对应的特征(天气)进行划分. 重复整个过程, 计算剩下三个特征的信息增益构造决策树.</p>
<p>信息增益率(C4.5):<br>    - 单纯使用信息增益虽然可以处理大部分场景, 但是当某一特征比较稀疏时, 比如上例中若加一个ID特征, 其值为(1 - 14). 那么这个特征就非常稀疏了, 按照它分类, 分类后的熵为0, 按照信息增益来讲这已经很理想了, 所以决定将ID作为根节点进行分类. 但其实ID对这个人会不会去打球并没有什么影响, 使用ID特征分类显然是不合乎实际的.<br>    - 信息增益率, 巧妙地避免了这种问题, 它不仅考虑信息增益, 还会考虑特征自身的熵. 信息增益率是信息增益和特征自身熵的比值, i.e., $$ informationGainRate_x &#x3D; \frac{informationGain_x}{entropy_x} $$</p>
<p>基尼系数:<br>    - CART决策树使用基尼系数做衡量标准.<br>    - 本质上和熵entropy是一样的.<br>    - 基尼系数越小, 不纯度越低, 特征越好. 这一点和信息增益相反.<br>    $$ gini(p) &#x3D; \sum_{k&#x3D;1}^K p_k (1 - p_k) &#x3D; 1 - \sum_{k&#x3D;1}^K p_k^2 $$</p>
<blockquote>
<p>Sklearn中的决策树</p>
<ul>
<li>sklearn.tree.DecisionTreeClassifier()</li>
<li>参数有:<br>1. criterion: 用于选择属性的准则, 可以传入”gini”代表基尼系数(默认值), “entropy”代表信息增益<br>2. max_features: 表示在决策树节点进行分裂时, 从多少个特征中选择最优特征. 可以设定固定数目, 百分比或者其他标准. 默认值是使用所有特征个数.</li>
</ul>
</blockquote>
<h4 id="随机森林算法"><a href="#随机森林算法" class="headerlink" title="随机森林算法"></a>随机森林算法</h4><p>Random Forest. 属于ensemble learning(集成算法).</p>
<p>ensemble learning可以看作是一种算法技巧, 目的是既然单个模型不太理想, 那么就群殴. 就训练多个模型, 去评测同一个样本, 根据不同的规则选择平均值或者少数服从多数等决定样本的类别或者值.</p>
<ul>
<li>bagging: 并行训练一堆分类器. 典型代表就是随机森林. 训练多个分类器取平均. $ f(x) &#x3D; \frac{1}{M} \sum_{m&#x3D;1}^M f_m(x) $. 随机体现在随机采样(从整个训练集中随机选取一定比例的样本)和随机选择特征(从所有的特征中随机选取一定比例的特征). 随机确保了分类的多样性, 这样取平均或者投票才有意义. 森林是指有很多个决策树并行放在一起.</li>
</ul>
<blockquote>
<p>Sklearn中的随机森林分类算法<br>sklearn.ensemble.RandomForestClassifier()<br>参数和方法:<br>      1. feature_importances_</p>
</blockquote>
<h4 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h4><ul>
<li>贝叶斯是为了解决逆向概率的. 逆向概率是指, 我们事先并不知到袋子里黑白球的比例, 而是闭着眼睛摸出一个(或几个)球, 观测这些取出来的球的颜色之后, 推测袋子里黑白球的比例.</li>
<li>$$ P(A|B) &#x3D; \frac{P(B|A)P(A)}{P(B)} $$</li>
<li></li>
</ul>
<h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><p>回归方法适合对一些带有时序信息的数据进行预测或者趋势进行拟合, 常用于金融及其他涉及时间序列的领域.</p>
<h3 id="评价指标-1"><a href="#评价指标-1" class="headerlink" title="评价指标"></a>评价指标</h3><p>均方误差. 回归任务通常使用的是均方误差作为评估指标, i.e.,</p>
<p>$$ E(f;D) &#x3D; \frac{1}{m}\sum_{i&#x3D;1}^m(f(x_i) - y_i)^2 $$</p>
<h1 id="Reference-List"><a href="#Reference-List" class="headerlink" title="Reference List"></a>Reference List</h1><ol>
<li><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/datasets.html">https://scikit-learn.org/stable/datasets.html</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/108393576">https://zhuanlan.zhihu.com/p/108393576</a></li>
</ol>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Reggie</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="https://reginald-l.github.io/2023/01/07/sklearn/">https://reginald-l.github.io/2023/01/07/sklearn/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2022 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span><strong>I am a slow walker, but I never walk backwards!!!</strong></span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"># 机器学习</a>
                    
                        <a href="/tags/Sklearn/"># Sklearn</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
            
            <a class="next" rel="next" href="/2023/01/02/AdaIN/">AdaIN</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span> Copyright © Reggie | 2022 </span>
    </div>
</footer>

    </div>
</body>

</html>