<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Reggie">





<title>DenseNet | Reggie&#39;s blog</title>



    <link rel="icon" href="/favicon.jpg">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    
    <script src="/js/heart.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 6.2.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Reggie&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Reggie&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6;    // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function () {
            tocbot.refresh(obj_merge(tocbot_default_config, { hasInnerContainers: true }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function () {
        tocbot.init(obj_merge(tocbot_default_config, { collapseDepth: 1 }));
    });

    function expandToc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, { collapseDepth: expanded ? 1 : DEPTH_MAX }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">DenseNet</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Reggie</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">August 4, 2022&nbsp;&nbsp;11:47:41</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="DenseNet概述"><a href="#DenseNet概述" class="headerlink" title="DenseNet概述"></a>DenseNet概述</h1><p>DenseNet是ResNet的改良版, 它在ImageNet上的表现要好于ResNet. 在ResNet中, Residual模块是通过 $加法$ 来连接正常通道和快速通道的, 而DenseNet则是通过在Channel维度上通过拼接来连接的. 如下图所示:</p>
<ul>
<li>左边的是ResNet的连接方式, 它使用 + 连接了A 和 B 模块</li>
<li>右边的是DenseNet的连接方式 (稠密连接), 它是在channel维度上拼接的A 和 B 模块<img src="/2022/08/04/DenseNet/ResNet%E5%92%8CDenseNet%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F.png" class="" title="ResNet和DenseNet连接方式"></li>
</ul>
<h1 id="稠密连接"><a href="#稠密连接" class="headerlink" title="稠密连接"></a>稠密连接</h1><p>DenseNet的这种连接方式称为稠密连接. 从数学角度可以表达为:<br>$$<br>X \to [X, f_1(X), f_2([X, f_1(X)]), f_3([X, f_1(X), f_2([X, f_1(X)])]), ……]<br>$$<br>稠密连接如下图所示:</p>
<img src="/2022/08/04/DenseNet/%E7%A8%A0%E5%AF%86%E8%BF%9E%E6%8E%A5.png" class="" title="稠密连接">

<h1 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h1><img src="/2022/08/04/DenseNet/DenseNet%E6%9E%B6%E6%9E%84.png" class="" title="DenseNet架构">
<p>DenseNet由两个部分组成: 稠密块(dense block)和过渡层(transition layer)</p>
<table>
<thead>
<tr>
<th></th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>Dense block</td>
<td>定义输入和输出是如何连接的</td>
</tr>
<tr>
<td>Transition layer</td>
<td>控制通道数, 使得模型不至于太大或者太复杂</td>
</tr>
</tbody></table>
<img src="/2022/08/04/DenseNet/DenseNet%E8%BF%87%E7%A8%8B.png" class="" title="DenseNet过程">
<blockquote>
<p>在DenseNet中, 他们的transition layers是由一个Batch normalization 层, 一个 $1 * 1$ 的卷积层 和 一个 $2 * 2$ 的平均池化层组成的</p>
</blockquote>
<h2 id="基本组件"><a href="#基本组件" class="headerlink" title="基本组件"></a>基本组件</h2><p>DenseNet使用的基本组件是ResNet的改良版”批量规范化, 激活和卷积”架构:</p>
<img src="/2022/08/04/DenseNet/%E7%BB%84%E4%BB%B6%E6%9E%B6%E6%9E%84.png" class="" title="组件架构">

<h3 id="卷积块组件代码的实现"><a href="#卷积块组件代码的实现" class="headerlink" title="卷积块组件代码的实现"></a>卷积块组件代码的实现</h3><p>这个组件是由 $BN \to ReLU \to 1*1 Conv \to BN \to ReLU \to 3 * 3 Conv$ 组成. 其中 $1 * 1$ 卷积层的输出通道数总是128, $3 * 3$ 卷积层的输出通道总是32 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def conv_block(in_channels):</span><br><span class="line">    return nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(in_channels), </span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(in_channels, 128, kernel_size=1, stride=1),</span><br><span class="line">        nn.BatchNorm2d(128),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(128, 32, kernel_size=3, stride=1, padding=1)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<h2 id="稠密块实现"><a href="#稠密块实现" class="headerlink" title="稠密块实现"></a>稠密块实现</h2><p>⼀个稠密块由多个卷积块组成, 每个卷积块使⽤相同数量的输出通道. 在前向传播中, 我们将每个卷积块的输⼊和输出在通道维上连结</p>
<img src="/2022/08/04/DenseNet/DenseNet%E5%AE%9E%E7%8E%B0%E8%AF%A6%E6%83%85.png" class="" title="DenseNet稠密块和过渡层细节">

<h3 id="稠密块的代码实现"><a href="#稠密块的代码实现" class="headerlink" title="稠密块的代码实现"></a>稠密块的代码实现</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class DenseBlock(nn.Module):</span><br><span class="line">    def __init__(self, in_channels, num_convs, growth_rate=32):</span><br><span class="line">        super().__init__()</span><br><span class="line">        layers = []</span><br><span class="line">        for i in range(num_convs):</span><br><span class="line">            # i是从0开始的, 不需要再 -1 了</span><br><span class="line">            layers.append(conv_block(in_channels + i * growth_rate))</span><br><span class="line">        self.net = nn.Sequential(*layers)</span><br><span class="line">    </span><br><span class="line">    def forward(self, X):</span><br><span class="line">        for block in self.net:</span><br><span class="line">            Y = block(X)</span><br><span class="line">            X = torch.cat((X, Y), dim=1)</span><br><span class="line">        return X</span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<h3 id="定义transition-layers"><a href="#定义transition-layers" class="headerlink" title="定义transition layers"></a>定义transition layers</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def trainsition_layer(in_channels, out_channels):</span><br><span class="line">    return nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(in_channels), </span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1),</span><br><span class="line">        nn.AvgPool2d(kernel_size=2, stride=2)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<h3 id="定义架构结构"><a href="#定义架构结构" class="headerlink" title="定义架构结构"></a>定义架构结构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_convs = (6, 12, 24, 16) # 表示每个dense block中有几个节点</span><br></pre></td></tr></table></figure>
<h3 id="定义各个阶段"><a href="#定义各个阶段" class="headerlink" title="定义各个阶段"></a>定义各个阶段</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">s1 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3),</span><br><span class="line">    nn.BatchNorm2d(64),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">s2 = nn.Sequential(DenseBlock(in_channels=64, num_convs=6), trainsition_layer(64+6*32, 128))</span><br><span class="line"></span><br><span class="line">s3 = nn.Sequential(DenseBlock(in_channels=128, num_convs=12), trainsition_layer(128+12*32, 256))</span><br><span class="line"></span><br><span class="line">s4 = nn.Sequential(DenseBlock(in_channels=256, num_convs=24), trainsition_layer(256+24*32, 512))</span><br><span class="line"></span><br><span class="line">s5 = nn.Sequential(DenseBlock(in_channels=512, num_convs=16))</span><br></pre></td></tr></table></figure>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    s1, s2, s3, s4, s5,</span><br><span class="line">    nn.AdaptiveAvgPool2d((1, 1)),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(1024, 1000)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="模型架构显示"><a href="#模型架构显示" class="headerlink" title="模型架构显示"></a>模型架构显示</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))</span><br><span class="line">    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">    (2): ReLU()</span><br><span class="line">    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (1): Sequential(</span><br><span class="line">    (0): DenseBlock(</span><br><span class="line">      (net): Sequential(</span><br><span class="line">        (0): Sequential(</span><br><span class="line">          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (1): Sequential(</span><br><span class="line">          (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (2): Sequential(</span><br><span class="line">          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (3): Sequential(</span><br><span class="line">          (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (4): Sequential(</span><br><span class="line">          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (5): Sequential(</span><br><span class="line">          (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Sequential(</span><br><span class="line">      (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">      (3): AvgPool2d(kernel_size=2, stride=2, padding=0)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (2): Sequential(</span><br><span class="line">    (0): DenseBlock(</span><br><span class="line">      (net): Sequential(</span><br><span class="line">        (0): Sequential(</span><br><span class="line">          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (1): Sequential(</span><br><span class="line">          (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (2): Sequential(</span><br><span class="line">          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (3): Sequential(</span><br><span class="line">          (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (4): Sequential(</span><br><span class="line">          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (5): Sequential(</span><br><span class="line">          (0): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (6): Sequential(</span><br><span class="line">          (0): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (7): Sequential(</span><br><span class="line">          (0): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (8): Sequential(</span><br><span class="line">          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (9): Sequential(</span><br><span class="line">          (0): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (10): Sequential(</span><br><span class="line">          (0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (11): Sequential(</span><br><span class="line">          (0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Sequential(</span><br><span class="line">      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">      (3): AvgPool2d(kernel_size=2, stride=2, padding=0)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (3): Sequential(</span><br><span class="line">    (0): DenseBlock(</span><br><span class="line">      (net): Sequential(</span><br><span class="line">        (0): Sequential(</span><br><span class="line">          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (1): Sequential(</span><br><span class="line">          (0): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (2): Sequential(</span><br><span class="line">          (0): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (3): Sequential(</span><br><span class="line">          (0): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (4): Sequential(</span><br><span class="line">          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (5): Sequential(</span><br><span class="line">          (0): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (6): Sequential(</span><br><span class="line">          (0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (7): Sequential(</span><br><span class="line">          (0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (8): Sequential(</span><br><span class="line">          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (9): Sequential(</span><br><span class="line">          (0): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (10): Sequential(</span><br><span class="line">          (0): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (11): Sequential(</span><br><span class="line">          (0): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (12): Sequential(</span><br><span class="line">          (0): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (13): Sequential(</span><br><span class="line">          (0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (14): Sequential(</span><br><span class="line">          (0): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (15): Sequential(</span><br><span class="line">          (0): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (16): Sequential(</span><br><span class="line">          (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (17): Sequential(</span><br><span class="line">          (0): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (18): Sequential(</span><br><span class="line">          (0): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (19): Sequential(</span><br><span class="line">          (0): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (20): Sequential(</span><br><span class="line">          (0): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (21): Sequential(</span><br><span class="line">          (0): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (22): Sequential(</span><br><span class="line">          (0): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (23): Sequential(</span><br><span class="line">          (0): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Sequential(</span><br><span class="line">      (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">      (3): AvgPool2d(kernel_size=2, stride=2, padding=0)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (4): Sequential(</span><br><span class="line">    (0): DenseBlock(</span><br><span class="line">      (net): Sequential(</span><br><span class="line">        (0): Sequential(</span><br><span class="line">          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (1): Sequential(</span><br><span class="line">          (0): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (2): Sequential(</span><br><span class="line">          (0): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (3): Sequential(</span><br><span class="line">          (0): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (4): Sequential(</span><br><span class="line">          (0): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (5): Sequential(</span><br><span class="line">          (0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (6): Sequential(</span><br><span class="line">          (0): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (7): Sequential(</span><br><span class="line">          (0): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (8): Sequential(</span><br><span class="line">          (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (9): Sequential(</span><br><span class="line">          (0): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (10): Sequential(</span><br><span class="line">          (0): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (11): Sequential(</span><br><span class="line">          (0): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (12): Sequential(</span><br><span class="line">          (0): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (13): Sequential(</span><br><span class="line">          (0): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (14): Sequential(</span><br><span class="line">          (0): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">        (15): Sequential(</span><br><span class="line">          (0): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (1): ReLU()</span><br><span class="line">          (2): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">          (4): ReLU()</span><br><span class="line">          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (5): AdaptiveAvgPool2d(output_size=(1, 1))</span><br><span class="line">  (6): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">  (8): Linear(in_features=1024, out_features=1000, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Reggie</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="https://reginald-l.github.io/2022/08/04/DenseNet/">https://reginald-l.github.io/2022/08/04/DenseNet/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2022 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span><strong>I am a slow walker, but I never walk backwards!!!</strong></span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"># 深度学习</a>
                    
                        <a href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"># 卷积神经网络</a>
                    
                        <a href="/tags/DenseNet/"># DenseNet</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2022/08/06/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%B9%BF/">数据增广</a>
            
            
            <a class="next" rel="next" href="/2022/08/03/ResNet/">ResNet</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span> Copyright © Reggie | 2022 </span>
    </div>
</footer>

    </div>
</body>

</html>