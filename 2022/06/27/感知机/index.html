<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Reggie">





<title>感知机 (perceptron) | Reggie&#39;s blog</title>



    <link rel="icon" href="/favicon.jpg">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    
    <script src="/js/heart.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 6.2.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Reggie&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Reggie&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6;    // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function () {
            tocbot.refresh(obj_merge(tocbot_default_config, { hasInnerContainers: true }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function () {
        tocbot.init(obj_merge(tocbot_default_config, { collapseDepth: 1 }));
    });

    function expandToc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, { collapseDepth: expanded ? 1 : DEPTH_MAX }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">感知机 (perceptron)</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Reggie</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">June 27, 2022&nbsp;&nbsp;15:46:13</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h1><h2 id="感知机的理解"><a href="#感知机的理解" class="headerlink" title="感知机的理解"></a>感知机的理解</h2><p>感知机 —– 一个典型的二分类模型.</p>
<p>给定输入向量 $X$, 权重向量 $W$ 和偏移标量 $b$, 感知机就能输出:<br>$$<br>    O &#x3D; \sigma {(&lt;W, X&gt; + b)} \quad \quad  \sigma{(x)} &#x3D; \begin{cases}1 &amp; if \quad x &gt; 0 \\ 0 &amp; otherwise \end{cases} \tag{1}<br>$$</p>
<img src="/2022/06/27/%E6%84%9F%E7%9F%A5%E6%9C%BA/%E6%84%9F%E7%9F%A5%E6%9C%BA.png" class="" title="感知机">

<p>从公式 $(1)$ 中可以看出, 感知机是由两部分组成, 一个是线性模型 $ W^T X $, 另一个是激活函数 (activation function). 公式 $(1)$ 中的 $\sigma$ 就是一个典型的激活函数.</p>
<ul>
<li>线性模型: 感知机是一个线性模型, 用于拟合一条直线或者一个平面将所有数据点正确地分成两个类别.</li>
<li>激活函数: 激活函数的目的是判断每一个数据点应该属于哪一个类别, 即应该是在直线的哪一册.</li>
</ul>
<p>感知机是一个典型的二分类模型.</p>
<p>二分类和线性回归, softmax回归的区别:</p>
<ul>
<li>线性回归和二分类虽然都只有一个输出, 但是线性回归的输出是一个实数 $R$; 而二分类的输出是一个离散的类别.</li>
<li>Softmax回归的输出个数是由分类类别的个数决定的, 可以是多个.</li>
</ul>
<h2 id="感知机的训练"><a href="#感知机的训练" class="headerlink" title="感知机的训练"></a>感知机的训练</h2><p>感知机的训练也是类似于随机梯度下降的.<br>$<br>initialize \quad w &#x3D; 0 \quad and \quad b &#x3D; 0 \\<br>repeat \\<br>\quad\quad if \quad y_i * [&lt;w, x_i&gt; + b] \leq 0 \quad then \\<br>\quad\quad\quad\quad        w \leftarrow w + y_i x_i \quad and \quad b \leftarrow b + y_i  \\<br>\quad\quad end \quad if \\<br>until \quad all \quad classified \quad correctly<br>$</p>
<img src="/2022/06/27/%E6%84%9F%E7%9F%A5%E6%9C%BA/%E6%84%9F%E7%9F%A5%E6%9C%BA%E8%AE%AD%E7%BB%83.png" class="" title="感知机训练过程">

<p>它的停止条件是所有的样本都被正确分类.</p>
<h2 id="收敛定理"><a href="#收敛定理" class="headerlink" title="收敛定理"></a>收敛定理</h2><img src="/2022/06/27/%E6%84%9F%E7%9F%A5%E6%9C%BA/%E6%94%B6%E6%95%9B%E5%AE%9A%E7%90%86.png" class="" title="收敛定理">

<p>收敛定理:</p>
<ul>
<li>数据在半径 r 内</li>
<li>余量 $\rho$ 使得: $ y(X^T W + b) \geq \rho \quad and \quad ||W||^2 + b^2 \leq 1$</li>
<li>感知机保证在 $\frac{r^2 + 1}{\rho ^ 2}$ 步后收敛.</li>
</ul>
<h2 id="XOR-问题-感知机的局限性"><a href="#XOR-问题-感知机的局限性" class="headerlink" title="XOR 问题 (感知机的局限性)"></a>XOR 问题 (感知机的局限性)</h2><img src="/2022/06/27/%E6%84%9F%E7%9F%A5%E6%9C%BA/XOR.png" class="" title="感知机的局限性">

<p>感知机不能拟合XOR函数, 因为它只能产生线性分割面.</p>
<p>所谓XOR问题就是异或问题, 当输入x和y都为1或者 -1 时 (即 x &#x3D; y), 样本属于-1类; 当输入x和y不同时, (即 x $ \neq $ y)时, 样本属于1类.</p>
<p>上图中红色表示一个类, 绿色表示一个类. 感知机是一个线性模型, 在二维平面中, 它没有办法用一条直线将红色和绿色完全分开.</p>
<h2 id="解决XOR问题的方法"><a href="#解决XOR问题的方法" class="headerlink" title="解决XOR问题的方法"></a>解决XOR问题的方法</h2><p>解决类似于XOR问题的方法常见的有三种:</p>
<ul>
<li>多层感知机: 将一个复杂的逻辑运算拆分为简单的 与或非, 然后每一个感知机负责计算一个简单的逻辑运算, 然后最后将结果汇集到一个感知机中计算复杂的逻辑运算. 如下图:<img src="/2022/06/27/%E6%84%9F%E7%9F%A5%E6%9C%BA/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5.png" class="" title="多层感知机"></li>
<li>核方法</li>
<li>向高维进行扩展: 这个的理解类似于将一座山从二维转为三维. 如下图:<img src="/2022/06/27/%E6%84%9F%E7%9F%A5%E6%9C%BA/kernel.png" class="" title="高维"></li>
</ul>
<h1 id="多层感知机-Multilayer-Perceptron-MLP"><a href="#多层感知机-Multilayer-Perceptron-MLP" class="headerlink" title="多层感知机 (Multilayer Perceptron MLP)"></a>多层感知机 (Multilayer Perceptron MLP)</h1><h2 id="多层感知机学习XOR"><a href="#多层感知机学习XOR" class="headerlink" title="多层感知机学习XOR"></a>多层感知机学习XOR</h2><img src="/2022/06/27/%E6%84%9F%E7%9F%A5%E6%9C%BA/%E5%AD%A6%E4%B9%A0XOR.png" class="" title="学习XOR">
<p>上图显示了多层感知机学习解决XOR问题的过程.</p>
<ul>
<li>首先将输入 $x$ 和 $y$ 输入到蓝色神经元中, 它对应左边图中的蓝线, 可以看出它将1和3划分为一个类别 (+), 2和4划分为另一个类别 (-);</li>
<li>接着将输入 $x$ 和 $y$ 输入到黄色神经元中, 它对应左边图中的黄线, 可以看出它将1和2划分为一个类别 (+), 3和4划分为另一个类别 (-);</li>
<li>最后将前两步中得到的结果作为输入到最后一个神经元中, 即可得到我们想要的正确的分类结果. 1和4一个类别 (+), 2和3一个类别 (-).</li>
</ul>
<h2 id="单隐藏层"><a href="#单隐藏层" class="headerlink" title="单隐藏层"></a>单隐藏层</h2><img src="/2022/06/27/%E6%84%9F%E7%9F%A5%E6%9C%BA/%E5%8D%95%E9%9A%90%E8%97%8F%E5%B1%82MLP.png" class="" title="单隐藏层MLP">
<p>值得注意的地方有:</p>
<ul>
<li>隐藏层的大小是一个超参数;</li>
<li>单隐藏层MLP的层数为2;</li>
<li>输入层与隐藏层, 隐藏层与输出层皆为全连接层.</li>
</ul>
<p>给定一个小批量样本 $X \in R^{n * d}$ , 其中 $n$ 为批量大小, $d$ 为输入个数. 假设隐藏层中的隐藏单元 (hidden unit) 个数为 $h$ . 记隐藏层的输出 (即隐藏层变量或者隐藏变量) 为 $H$ , 则有 $H \in R ^ {n*h}$ . 同样滴, 隐藏层的权重 $W_h$ 的形状为 $(d * h)$ , 偏置项 $ b_h $ 的形状为 $(1 * h)$ . 记输出层的神经元个数为 $q$ , 则有输出层的权重 $W_o \in R^{h * q}$, 偏置项 $b_o \in R^{1 * q}$. 基于此, 多层感知机的输出 $O \in R^{n * q}$ 的计算过程如下:</p>
<p>$$ H &#x3D; XW_h + b_h \\ O &#x3D; HW_o + b_o \tag{2}$$<br>其形状变化过程如下:<br>$$ (n * h) &lt;&#x3D; (n * d) (d * h) + (1 * h) \\ (n * q) &lt;&#x3D; (n * h) (h * q) + (1 * q) $$</p>
<p>一些重要的术语:</p>
<table>
<thead>
<tr>
<th>术语</th>
<th>英文(别名)</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td>隐藏单元</td>
<td>hidden unit</td>
<td>隐藏层的神经元个数</td>
</tr>
<tr>
<td>隐藏变量</td>
<td>隐藏层变量</td>
<td>隐藏层的输出个数</td>
</tr>
</tbody></table>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>在上面公式 (2) 中, 我们将两个式子联立可以得到:<br>$$ O &#x3D; (XW_h + b_h)W_o + b_o &#x3D; XW_hW_o + b_hW_o + b_o \tag{3}$$<br>从公式(3)可以看出, 虽然引入了隐藏层, 但是它的本质依然是一个线性模型, 等价于一个权重为 $(W_hW_o)$ , 偏置为 $b_hW_o + b_o$ 的单层神经网络. 即使添加再多的隐藏层, 这种设计依然只能与仅含输出层和输入层的单层神经网络等价.</p>
<p>这种问题的主要原因是全连接层只对数据进行了仿射变换(affine transformation), 再多的仿射变换函数叠加, 得到的依然是一个简单的仿射变换. 为了解决这个问题, 需要在隐藏层引入一个非线性变换函数, 例如对隐藏变量用按元素运算的非线性函数进行变换, 然后再作为下一个全连接层的输入.这就是激活函数的作用与目的.</p>
<p>激活函数可以有效地避免神经层的塌陷.</p>
<p>常见的激活函数:</p>
<ul>
<li>ReLU函数</li>
<li>sigmoid函数</li>
<li>tanh函数</li>
</ul>
<p>引入激活函数后, 单分类多层感知机可以改写为:<br>$$ H &#x3D; \phi(XW_h + b_h) \\ O &#x3D; HW_o + b_o \tag{3}$$</p>
<p>多类分类多层感知机为:<br>$$ H &#x3D; \phi(XW_h + b_h) \\ O &#x3D; HW_o + b_o \\ y&#x3D;softmax(O) \tag{4}$$</p>
<h2 id="多隐藏层MLP"><a href="#多隐藏层MLP" class="headerlink" title="多隐藏层MLP"></a>多隐藏层MLP</h2><img src="/2022/06/27/%E6%84%9F%E7%9F%A5%E6%9C%BA/%E5%A4%9A%E9%9A%90%E8%97%8F%E5%B1%82MLP.png" class="" title="多隐藏层MLP">

<ul>
<li>隐藏层的个数是一个超参数</li>
<li>每一个隐藏层的神经元个数也是一个超参数</li>
<li>每一个隐藏层都有对应的权重矩阵和偏置向量</li>
<li>每一个隐藏层都有对应的激活函数</li>
</ul>
<p>机器学习的本质就是一个压缩的过程, 通常是将一个高维的东西压缩为一个低维度的东西, 比如单分类图像问题, 就是将一个2维图像转为一个类别输出; 多分类FashionMNIST就是将784(28*28)的高维转为10个类别的输出. 所谓的压缩可以直观地解释为输入的个数较多, 而输出的个数远远少于输入的个数. 在多隐藏层MLP中, 我们通常是将第一个隐藏层设置的较大, 也就是宽度较大, 然后逐级递减, 慢慢向着最终想要的输出的个数逼近. 如果将第一个隐藏层设置的太小就会损失掉太多的空间或者时序信息.</p>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><h2 id="从零开始实现"><a href="#从零开始实现" class="headerlink" title="从零开始实现"></a>从零开始实现</h2><h3 id="1-导入环境"><a href="#1-导入环境" class="headerlink" title="1. 导入环境"></a>1. 导入环境</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.utils import data</span><br><span class="line">from torch import nn</span><br><span class="line">import torchvision</span><br><span class="line">from torchvision import transforms as transforms</span><br><span class="line">from d2l import torch as d2l</span><br></pre></td></tr></table></figure>

<h3 id="2-数据集下载和data-iter构建"><a href="#2-数据集下载和data-iter构建" class="headerlink" title="2. 数据集下载和data_iter构建"></a>2. 数据集下载和data_iter构建</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">batch_size = 256</span><br><span class="line"></span><br><span class="line">trans = transforms.Compose([transforms.ToTensor()])</span><br><span class="line">train_data = torchvision.datasets.FashionMNIST(root=&#x27;../data/&#x27;, train=True, transform=trans, download=False)</span><br><span class="line">test_data = torchvision.datasets.FashionMNIST(root=&#x27;../data/&#x27;, train=False, transform=trans, download=False)</span><br><span class="line"></span><br><span class="line">train_iter = data.DataLoader(train_data, batch_size, shuffle=True)</span><br><span class="line">test_iter = data.DataLoader(test_data, batch_size, shuffle=True)</span><br></pre></td></tr></table></figure>

<h3 id="3-模型参数"><a href="#3-模型参数" class="headerlink" title="3. 模型参数"></a>3. 模型参数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># num_hiddens 是介于num_inputs和num_outputs之间的</span><br><span class="line">num_inputs, num_outputs, num_hiddens = 784, 10, 256</span><br><span class="line"></span><br><span class="line">W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True))</span><br><span class="line">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))</span><br><span class="line"></span><br><span class="line">W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs), requires_grad=True)</span><br><span class="line">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br></pre></td></tr></table></figure>

<h3 id="4-激活函数"><a href="#4-激活函数" class="headerlink" title="4. 激活函数"></a>4. 激活函数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># ReLU激活函数</span><br><span class="line">def relu(X):</span><br><span class="line">    a = torch.zeros_like(X)</span><br><span class="line">    return torch.max(X, a)</span><br></pre></td></tr></table></figure>

<h3 id="5-模型定义"><a href="#5-模型定义" class="headerlink" title="5. 模型定义"></a>5. 模型定义</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 定义模型</span><br><span class="line">def net(X):</span><br><span class="line">    # print(f&#x27;X.shape = &#123;X.shape&#125;&#x27;)</span><br><span class="line">    X = X.reshape(-1, num_inputs)</span><br><span class="line">    # print(f&#x27;X,shape1 = &#123;X.shape&#125;&#x27;)</span><br><span class="line">    # print(f&#x27;W1.shape = &#123;W1.shape&#125;&#x27;)</span><br><span class="line">    # print(f&#x27;b1.shape = &#123;b1.shape&#125;&#x27;)</span><br><span class="line">    H = relu(X @ W1 + b1)</span><br><span class="line">    return (H @ W2 + b2)</span><br></pre></td></tr></table></figure>

<h3 id="6-损失函数和优化算法"><a href="#6-损失函数和优化算法" class="headerlink" title="6. 损失函数和优化算法"></a>6. 损失函数和优化算法</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = 10, 0.1</span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">updater = torch.optim.SGD(params, lr)</span><br></pre></td></tr></table></figure>

<h3 id="7-训练"><a href="#7-训练" class="headerlink" title="7. 训练"></a>7. 训练</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br></pre></td></tr></table></figure>

<h2 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h2><h3 id="1-环境导入"><a href="#1-环境导入" class="headerlink" title="1. 环境导入"></a>1. 环境导入</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.utils import data</span><br><span class="line">from torch import nn</span><br><span class="line">import torchvision</span><br><span class="line">from torchvision import transforms as transforms</span><br><span class="line">from d2l import torch as d2l</span><br></pre></td></tr></table></figure>

<h3 id="2-数据集下载和构建data-iter"><a href="#2-数据集下载和构建data-iter" class="headerlink" title="2. 数据集下载和构建data_iter"></a>2. 数据集下载和构建data_iter</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">batch_size = 256</span><br><span class="line">trans = transforms.Compose([transforms.ToTensor()])</span><br><span class="line">train_data = torchvision.datasets.FashionMNIST(root=&#x27;../data/&#x27;, train=True, transform=trans, download=False)</span><br><span class="line">test_data = torchvision.datasets.FashionMNIST(root=&#x27;../data/&#x27;, train=False, transform=trans, download=False)</span><br><span class="line">train_iter = data.DataLoader(train_data, batch_size, shuffle=True)</span><br><span class="line">test_iter = data.DataLoader(test_data, batch_size, shuffle=True)</span><br></pre></td></tr></table></figure>

<h3 id="3-模型定义"><a href="#3-模型定义" class="headerlink" title="3. 模型定义"></a>3. 模型定义</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Flatten(), </span><br><span class="line">    nn.Linear(num_inputs, num_hiddens), </span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(num_hiddens, num_outputs)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">def init_weights(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=0.1)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure>

<h3 id="4-损失函数和优化算法"><a href="#4-损失函数和优化算法" class="headerlink" title="4. 损失函数和优化算法"></a>4. 损失函数和优化算法</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">batch_size = 256</span><br><span class="line">lr = 0.1</span><br><span class="line">num_epochs = 10</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr)</span><br></pre></td></tr></table></figure>

<h3 id="5-训练"><a href="#5-训练" class="headerlink" title="5. 训练"></a>5. 训练</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>
        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Reggie</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="https://reginald-l.github.io/2022/06/27/%E6%84%9F%E7%9F%A5%E6%9C%BA/">https://reginald-l.github.io/2022/06/27/%E6%84%9F%E7%9F%A5%E6%9C%BA/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2022 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span><strong>I am a slow walker, but I never walk backwards!!!</strong></span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/"># 感知机</a>
                    
                        <a href="/tags/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"># 多层感知机</a>
                    
                        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"># 深度学习</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2022/07/01/%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88/">模型选择与过拟合和欠拟合</a>
            
            
            <a class="next" rel="next" href="/2022/06/20/%E6%95%B0%E6%8D%AE%E9%9B%86/">数据集</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span> Copyright © Reggie | 2022 </span>
    </div>
</footer>

    </div>
</body>

</html>