"use strict";(self.webpackChunkreggie_blog=self.webpackChunkreggie_blog||[]).push([[307],{3749:n=>{n.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"Lora","metadata":{"permalink":"/Lora","source":"@site/blog/lora/index.md","title":"Lora","description":"\u4e3a\u4ec0\u4e48LoRA\u53ef\u4ee5work?","date":"2024-05-18T07:39:48.415Z","tags":[{"label":"Lora","permalink":"/tags/lora"},{"label":"stable diffusion","permalink":"/tags/stable-diffusion"}],"hasTruncateMarker":true,"authors":[{"name":"Reggie","url":"https://github.com/Reginald-L","imageURL":"https://avatars.githubusercontent.com/u/67569154?s=400&u=f5ac24204bcae51744cbed83d7afde7f7551aa16&v=4","key":"Reggie"}],"frontMatter":{"slug":"Lora","title":"Lora","authors":["Reggie"],"tags":["Lora","stable diffusion"]},"unlisted":false},"content":"## \u4e3a\u4ec0\u4e48LoRA\u53ef\u4ee5work?\\n1. \u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f4e\u79e9\u7279\u6027: \u5728\u9002\u914d\u4e0b\u6e38\u4efb\u52a1\u65f6, \u7531\u4e8e\u5927\u6a21\u578b\u5b58\u5728\u4e00\u4e2a\u6f5c\u5728\u7684\u4f4e\u7ef4\\"\u5185\u5728\u7ef4\u5ea6\\"(instrisic dimension), \u6240\u4ee5\u5373\u4f7f\u628a\u5b83\u4eec\u968f\u673a\u6620\u5c04\u5230\u4e00\u4e2a\u8f83\u5c0f\u7684\u5b50\u7a7a\u95f4, \u4ed6\u4eec\u4f9d\u7136\u53ef\u4ee5\u9ad8\u6548\u51c6\u786e\u5730\u5b66\u4e60\u65b0\u6570\u636e\u7684\u7279\u6027. LoRA\u5c31\u662f\u501f\u9274\u7684\u8fd9\u4e2d\u4f4e\u79e9\u601d\u60f3, \u5373\u5728\u9002\u914d\u4e0b\u6e38\u4efb\u52a1\u65f6, \u53ef\u4ee5\u7ed9\u9884\u8bad\u7ec3\u7684\u5927\u6a21\u578b\u6ce8\u5165\u4e00\u4e2a\u4f4e\u79e9\u7684\u53ef\u8bad\u7ec3\u7684\u53c2\u6570\u77e9\u9635\u5c31\u53ef\u4ee5\u7528\u8f83\u5c11\u7684\u53c2\u6570\u6355\u6349\u5230\u7279\u5b9a\u6570\u636e\u96c6\u7684\u4e3b\u8981\u7279\u6027.\\n2. \u4fdd\u6301\u9884\u8bad\u7ec3\u6743\u91cd\u4e0d\u53d8, \u5927\u6a21\u578b\u5fae\u8c03\u5f80\u5f80\u4f1a\u5bfc\u81f4\u539f\u6709\u6a21\u578b\u51fa\u73b0\\"catastrophic forgetting\\"\u95ee\u9898,\u4f46\u662fLoRA\u5728\u9002\u914d\u4e0b\u6e38\u4efb\u52a1\u65f6\u51bb\u7ed3\u4e86\u539f\u6709\u53c2\u6570, \u786e\u4fdd\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u9002\u914d\u8fc7\u7a0b\u4e2d\u4f9d\u7136\u4fdd\u6301\u9884\u8bad\u7ec3\u9636\u6bb5\u5b66\u5230\u7684\u80fd\u529b.\\n3. \u7075\u6d3b\u6027\u548c\u6613\u5206\u4eab\u6027: \u591a\u4e2aLoRA\u53ef\u4ee5\u7ec4\u5408\u4f7f\u7528\\n4. \u65e0\u63a8\u7406\u5ef6\u8fdf: LoRA\u4e0d\u4f1a\u5728\u6a21\u578b\u63a8\u7406\u9636\u6bb5\u5f15\u5165\u989d\u5916\u7684\u65f6\u95f4\u5f00\u9500, \u8fd9\u4e0eadapter\u8fd9\u79cd\u5fae\u8c03\u65b9\u5f0f\u6709\u7740\u5f88\u5927\u7684\u4f18\u52bf\\n   \\n\x3c!-- truncate --\x3e\\n\\n## \u77e9\u9635\u521d\u59cb\\n![image.png](./images/paper_image.png)\\nA: \u4f7f\u7528\u968f\u673a\u9ad8\u65af\u521d\u59cb\u5316(Random Gaussian Initialization): lora_down\\n```python\\nnn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\\n```\\nB: Zero \u521d\u59cb\u5316: lora_up\\n```python\\nnn.init.zeros_(self.lora_B)\\n```\\n\\n## \u516c\u5f0f\\n$$\\nh = W_0 \\\\cdot x \\\\stackrel{\u5fae\u8c03}{\\\\longrightarrow} W_0 \\\\cdot x + \\\\nabla W \\\\cdot x \\\\stackrel{LoRA}{\\\\longrightarrow} W_0 \\\\cdot x + BA \\\\cdot x\\n$$\\n\u901a\u5e38\u4f1a\u5728LoRA\u90e8\u5206\u6dfb\u52a0\u4e00\u4e2a\u7f29\u653e\u7cfb\u6570 scale, \u5373\\n\\n$$\\nh = W_0 \\\\cdot x + scale \\\\cdot BA \\\\cdot x\\n$$\\n\\n\u5f53scale \u7cfb\u6570\u03b1\u4e3a0\u5219\u5b8c\u5168\u4e0d\u4f7f\u7528LORA, \u53ea\u662f\u7528\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u53c2\u6570.\\n\\n\u5fae\u8f6f\u7684\u4ee3\u7801\u4e2d\u4f7f\u7528\u4f1a\u6709\u4e00\u4e2a\u989d\u5916\u7684lora_alpha\u6765\u8ba1\u7b97\u7f29\u653e\u7cfb\u6570, \u5373\\n$$\\nscale = \\\\frac{lora\\\\_alpha}{lora\\\\_rank} = \\\\frac{\\\\alpha} {rank}\\n$$\\n\\n\\n## LoRA\u9009\u62e9\u90a3\u4e2a\u77e9\u9635\u8fdb\u884c\u4f4e\u79e9\u5206\u89e3\\n\\n1. \u7406\u8bba\u4e0a\u4efb\u4f55\u4e00\u4e2a\u77e9\u9635\u90fd\u53ef\u4ee5\u88ab\u4f4e\u79e9\u5206\u89e3. \u5728Transformer\u67b6\u6784\u4e2d, \u4f5c\u8005\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0, \u53ea\u5bf9attention\u90e8\u5206\u8fdb\u884clora\u4f4e\u79e9\u6ce8\u5165\u5c31\u80fd\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u83b7\u5f97\u6bd4\u8f83\u597d\u7684\u6548\u679c.\\n2. \u5355\u72ec\u5bf9Wq\u6216\u8005Wk\u8fdb\u884c\u5206\u89e3\u5f97\u5230\u4e86\u4e00\u4e2a\u975e\u5e38\u4f4e\u7684\u503c,\u4f46\u662f\u901a\u8fc7\u5bf9Wq\u548cWv\u5219\u6d3b\u5f97\u4e86\u6700\u597d\u7684\u8868\u73b0\\n\\n## \u4ee3\u7801\\n![lora.png](./images/lora.png)\\n### \u6ce8\u5165\u7ebf\u6027\u5c42\\n```python\\nclass LoraInjectedLinear(nn.Module):\\n    def __init__(\\n        self, in_features, out_features, bias=False, r=4, dropout_p=0.1, scale=1.0\\n    ):\\n        super().__init__()\\n\\n        if r > min(in_features, out_features):\\n            raise ValueError(\\n                f\\"LoRA rank {r} must be less or equal than {min(in_features, out_features)}\\"\\n            )\\n        self.r = r\\n        # w\\n        self.linear = nn.Linear(in_features, out_features, bias)\\n        # lora_down\\n        self.lora_down = nn.Linear(in_features, r, bias=False)\\n        # lora_up\\n        self.lora_up = nn.Linear(r, out_features, bias=False)\\n        \\n        self.dropout = nn.Dropout(dropout_p)\\n        \\n        self.scale = scale\\n        self.selector = nn.Identity()\\n\\n        nn.init.normal_(self.lora_down.weight, std=1 / r)\\n        nn.init.zeros_(self.lora_up.weight)\\n\\n    def forward(self, input):\\n        return (\\n            self.linear(input)\\n            + self.dropout(self.lora_up(self.selector(self.lora_down(input))))\\n            * self.scale\\n        )\\n```\\n### \u6ce8\u5165Conv2d\u5c42\\n```python\\nclass LoraInjectedConv2d(nn.Module):\\n    def __init__(\\n        self, in_channels: int, out_channels: int, kernel_size,\\n        stride=1, padding=0, dilation=1, groups: int = 1, bias: bool = True,\\n        r: int = 4, dropout_p: float = 0.1, scale: float = 1.0,\\n    ):\\n        super().__init__()\\n        if r > min(in_channels, out_channels):\\n            raise ValueError(\\n                f\\"LoRA rank {r} must be less or equal than {min(in_channels, out_channels)}\\"\\n            )\\n        self.r = r\\n        # W\\n        self.conv = nn.Conv2d(\\n            in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\\n            stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias,\\n        )\\n        # lora_down\\n        self.lora_down = nn.Conv2d(\\n            in_channels=in_channels, out_channels=r, kernel_size=kernel_size,\\n            stride=stride, padding=padding, dilation=dilation, groups=groups, bias=False,\\n        )\\n        # lora_up\\n        self.lora_up = nn.Conv2d(\\n            in_channels=r, out_channels=out_channels, kernel_size=1, \\n            stride=1, padding=0, bias=False,\\n        )\\n        self.dropout = nn.Dropout(dropout_p)\\n        self.selector = nn.Identity()\\n        self.scale = scale\\n\\n        nn.init.normal_(self.lora_down.weight, std=1 / r)\\n        nn.init.zeros_(self.lora_up.weight)\\n\\n    def forward(self, input):\\n        return (\\n            self.conv(input)\\n            + self.dropout(self.lora_up(self.selector(self.lora_down(input))))\\n            * self.scale\\n        )\\n```"}]}}')}}]);